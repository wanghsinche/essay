# 重新入门神经网络·简单的前馈神经网络

> 学习神经网络最好的办法就是从头推导并用python实现一遍最简单的前馈神经网络FNN（forward feed neural network）

GitHub的人工智能copilot最近帮我补全了很多代码。在惊喜之余，我不免对人工智能替代程序员产生担忧。为了一探究竟，决定认真学习一下机器学习里面最火的一个领域----神经网络。

看了前两章《neural network and deep learning》之后，我感觉自己学到不少。因此也决定从头推导出神经网络模型的训练算法，也即大名鼎鼎的back propagation反向传播算法。

## 一、基本定义和训练流程

FNN分类器是一种简单的神经网络模型：一个FNN神经网络接受多个输入，最终输出结果，实现对输入数据的分类。

所以训练FNN神经网络和拟合线性回归模型从原理上讲没什么区别：

**定义损失函数**-->**梯度下降**-->**得到令损失函数最小的模型参数**

如果定义神经网络模型，令第l层的第i个神经元<img src="https://render.githubusercontent.com/render/math?math=z_i^l" > ，有

(1-1)

<img src="https://render.githubusercontent.com/render/math?math=z_i^l = \sum_{k=1}^nw_{ki}^{l-1}a_k^{l-1} + b_i^l" > 

经过激活函数<img src="https://render.githubusercontent.com/render/math?math=\sigma(z)">后，其输出为：

(1-2)

<img src="https://render.githubusercontent.com/render/math?math=a_i^l = \sigma(z_i^l)">      

那么对于这个模型，可以定义它的损失函数L为：

(1-3)

<img src="https://render.githubusercontent.com/render/math?math=L = \frac{1}{2n}\sum_{k=1}^n(f(x_k)-y_k)^2">      

其中 f(x) 为神经网络的输出，y是x对应的目标值。

为了得到令损失函数L最小的模型参数<img src="https://render.githubusercontent.com/render/math?math=w_{ki}^l">和<img src="https://render.githubusercontent.com/render/math?math=b_i^l">，可以对损失函数L使用梯度下降法。

## 二、梯度的求解和反向传播算法back propagation

> 神经网络结构过于复杂，直接对<img src="https://render.githubusercontent.com/render/math?math=w_{ki}^l">和<img src="https://render.githubusercontent.com/render/math?math=b_i^l">求导几乎无法完成。幸好上世纪机器学习的先驱们发明了反向传播算法，可以相当快速的求解梯度。

反向传播算法的核心是灵活运用微分中的链式法则chain-rule，并巧妙的定义了中间变量。

----
### A. 梯度的求解流程

首先需要求出L的梯度<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial w_{ki}^l}">和<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l}">

(2-1) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial w_{ki}^l} = \frac{1}{2n} \frac{\partial{\sum_{k=1}^n(f(x_k)-y_k)^2}}{\partial w_{ki}^l} = \frac{1}{2n}\left( \frac{\partial{((f(x_1)-y_1)^2 + ... + f(x_n)-y_n)^2)}}{\partial w_{ki}^l}\right) ">

类似的 

(2-2) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l} = \frac{1}{2n} \frac{\partial{\sum_{k=1}^n(f(x_k)-y_k)^2}}{\partial b_i^l} = \frac{1}{2n}\left( \frac{\partial{((f(x_1)-y_1)^2 + ... + f(x_n)-y_n)^2)}}{\partial b_i^l}\right) = \frac{1}{n}\left( \frac{\partial{(\frac{(f(x_1)-y_1)^2}{2} + ... + \frac{(f(x_n)-y_n)^2}{2})}}{\partial b_{i}^l}\right)">

为简化计算，对于一样本数据(x,y)，定义

(2-3) 

<img src="https://render.githubusercontent.com/render/math?math=C = \frac{(f(x)-y)^2}{2}" >

那么

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l}  = \frac{1}{2n}\left( \frac{\partial{((f(x_1)-y_1)^2 + ... + f(x_n)-y_n)^2)}}{\partial b_i^l}\right) = \frac{1}{n}\left( \frac{\partial{(\frac{(f(x_1)-y_1)^2}{2} + ... + \frac{(f(x_n)-y_n)^2}{2})}}{\partial b_i^l}\right)" >

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l}=\frac{1}{n}\left(\frac{\partial (C(x_1)+...+C(x_n))}{\partial b_i^l}\right)" >

(2-1-1)

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l}=\frac{1}{n}\left(\frac{\partial (C(x_1)}{\partial b_i^l}+...+\frac{\partial (C(x_n)}{\partial b_i^l}\right)" >

类似的 

(2-2-1)

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial w_{ki}^l}=\frac{1}{n}\left(\frac{\partial (C(x_1)}{\partial w_{ki}^l}+...+\frac{\partial (C(x_n)}{\partial w_{ki}^l}\right)" />


所以只要求出C对于<img src="https://render.githubusercontent.com/render/math?math=w_{ki}^l">和<img src="https://render.githubusercontent.com/render/math?math=b_i^l"/>的梯度，然后将一批样本数据带入求和即可。

---

### B. 求各自的梯度

根据(1-1) <img src="https://render.githubusercontent.com/render/math?math=w_{ki}^l">和<img src="https://render.githubusercontent.com/render/math?math=z_i^l"/> 的定义，函数C的梯度为

(2-4)

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial w_{ki}^l} = \frac{\partial C}{\partial z_i^{l+1}}\frac{\partial z_i^{l+1}}{\partial w_{ki}}" />


注意是 i+1

(2-5) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial b_i^l}= \frac{\partial C}{\partial z_i^l}\frac{\partial z_i^l}{\partial b_i^l}"/>

注意是 i

所以简单求出


(2-6) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial z_i^{l+1}}{\partial w_{ki}^l} = a_k^l = \sigma(z_i^l)"/>


(2-7) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial z_i^l}{\partial b_i^l} = 1"
/>


因此剩下一个未知项 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial z_i^l} = ?" />

---

### C. 反向传播back propagation

利用反向传播算法可以迭代求出 <img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial z_i^l}" />：

首先函数C可以表示为对于第i+1层的所有神经元z的多变量函数

(3-1)

<img src="https://render.githubusercontent.com/render/math?math=C = C(z_1^{l+1}, z_2^{l+1},...,z_n^{l+1})" />

而根据(1-1)已经知道

(3-2)

<img src="https://render.githubusercontent.com/render/math?math=z^{l+1} = g(z_1^l, z_2^l,..., z_n^l)" />



应用多变量的链式法则可以得到

(3-3)


<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial z_i^l} = \frac{\partial {C(z_1^{l+1}, z_2^{l+1},...,z_n^{l+1})}}{\partial z_i^l}= \frac{\partial C}{\partial z_1^{l+1}}\frac{\partial z_1^{l+1}}{\partial z_i^l} + ... + \frac{\partial C}{\partial z_n^{l+1}}\frac{\partial z_n^{l+1}}{\partial z_i^l}= \sum_{k=1}^n \frac{\partial {C}}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_i^l} " />


如果<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial z_i^l}">记作<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^l"/>，则(3-3)就是

(3-4) 

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^l = \sum_{k=1}^n \epsilon_k^{l+1} \frac{\partial z_k^{l+1}}{\partial z_i^l}"/>

而根据(1-1)，(1-2)，使用链式法则可以轻易求出


(3-5)

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial z_i^{l+1}}{\partial z_k^l} =  \frac{\partial z_i^{l+1}}{\partial a_k^l} \frac{\partial a_k^l}{\partial z_k^l}=w_{ki}^l \sigma'(z_k^l)"/>


所以递推公式为

(3-6) 

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^l = \sum_{k=1}^n \epsilon_k^{l+1} w_{ki}^l \sigma'(z_i^l)"/>

---
### D. 前馈传播求出最后一层<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^{out}"/>

对于递推公式(3-6)需要找到初始值<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^{out}"/>，这里可以用神经网络直接代入x求解：

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^{out} = \frac{\partial C}{\partial{z_i^{out}}} "/>

<img src="https://render.githubusercontent.com/render/math?math==\frac{\partial((f(x)-y)^2/2)}{\partial z_i^{out}} "/>

<img src="https://render.githubusercontent.com/render/math?math== \frac{\partial((f(a_1^{out}, a_2^{out},..., a_n^{out})-y)^2/2)}{\partial z_i^{out}}"/>

<img src="https://render.githubusercontent.com/render/math?math== \frac{\partial((f(a_1^{out}, a_2^{out},..., a_n^{out})-y)^2/2)}{\partial a_i^{out}}\frac{\partial a_i^{out}}{\partial z_i^{out}}"/>

<img src="https://render.githubusercontent.com/render/math?math== (f-y) \frac{\partial f}{\partial a_i^{out}} \sigma'(z_i^{out})"/>


由于<img src="https://render.githubusercontent.com/render/math?math=a_i^{out}"/>就是最后一层，即神经网络f的最终输出，所以<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial f}{\partial a_i^{out}} = 1"/> 

最终

(3-8) 

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^{out} = (f-y)\sigma'(z_i^{out})"/>

### E. 结合起来完成推导

(4-1) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial w_{ki}^l} = \epsilon_i^l\frac{\partial z_i^{l+1}}{\partial w_{ki}^l}= \epsilon_i^l \sigma(z_i^l)"/>


(4-2) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial C}{\partial b_i^l} = \epsilon_i^l"/>


(3-6) 

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^l = \sum_{k=1}^n \epsilon_k^{l+1} w_{ki}^l \sigma'(z_i^l)"/>


(3-8) 
<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^{out} = (f-y)\sigma'(z_i^{out})"/>


所以$L$的梯度<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial w_{ki}^l}"/>，<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l}"/>为

(4-3) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial w_{ki}^l} = \frac{1}{n}\sum_{k=1}^n \frac{\partial C(x_k)}{\partial w_{ki}^l}"/>


(4-4) 

<img src="https://render.githubusercontent.com/render/math?math=\frac{\partial L}{\partial b_i^l} = \frac{1}{n}\sum_{k=1}^n \frac{\partial C(x_k)}{\partial b_i^l}" />


## 三、根据推导出的公式用numpy实现前馈神经网络

为了检验学到的知识，利用推导出的公式实现一个3层的简单前馈神经网络，用于预测sklearn自带的鸢尾花数据集分类。

To be continue...
